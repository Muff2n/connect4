{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(999)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 2\n",
    "filters = 64\n",
    "\n",
    "n_epocs = 10000\n",
    "batch_size = 4096\n",
    "test_size = 0.2\n",
    "learning_rate = 0.001 * (batch_size / 1024.0)\n",
    "momentum = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Dataset(data.Dataset):\n",
    "    def __init__(self, boards, values):\n",
    "        assert len(boards) == len(values)\n",
    "        self.boards = boards\n",
    "        self.values = values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.boards)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        value = self.values[idx].item()\n",
    "        if value == 1.0:\n",
    "            a = 0\n",
    "        elif value == 0.5:\n",
    "            a = 1\n",
    "        elif value == 0.0:\n",
    "            a = 2\n",
    "        return self.boards[idx], a\n",
    "\n",
    "boards = torch.load('/home/richard/Downloads/connect4_boards.pth').numpy()\n",
    "values = torch.load('/home/richard/Downloads/connect4_values.pth').numpy()\n",
    "\n",
    "# Here we don't want to have the player to move channel\n",
    "boards = boards[:, 3 - channels:]\n",
    "\n",
    "board_train, board_test, value_train, value_test = train_test_split(boards, values, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54045 54045\n"
     ]
    }
   ],
   "source": [
    "# augment data\n",
    "np.random.shuffle(value_train)\n",
    "np.random.shuffle(board_train)\n",
    "\n",
    "print(len(value_train), len(board_train))\n",
    "\n",
    "train = Connect4Dataset(torch.from_numpy(board_train), torch.from_numpy(value_train))\n",
    "test = Connect4Dataset(torch.from_numpy(board_test), torch.from_numpy(value_test))\n",
    "\n",
    "train_gen = data.DataLoader(train, batch_size, shuffle=False)\n",
    "test_gen = data.DataLoader(test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): ResidualLayer(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): ResidualLayer(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): ResidualLayer(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): ResidualLayer(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (2): ValueHead_Classifier(\n",
       "    (conv1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (batch_norm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.01)\n",
       "    (fcN): Sequential(\n",
       "      (0): Linear(in_features=42, out_features=42, bias=True)\n",
       "      (1): Linear(in_features=42, out_features=42, bias=True)\n",
       "      (2): Linear(in_features=42, out_features=42, bias=True)\n",
       "      (3): Linear(in_features=42, out_features=42, bias=True)\n",
       "    )\n",
       "    (fc2): Linear(in_features=42, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.connect4.utils import NetworkStats as info\n",
    "\n",
    "convolutional_layer = \\\n",
    "    nn.Sequential(nn.Conv2d(in_channels=channels,\n",
    "                            out_channels=filters,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1,\n",
    "                            dilation=1,\n",
    "                            groups=1,\n",
    "                            bias=False),\n",
    "                  nn.BatchNorm2d(filters),\n",
    "                  nn.LeakyReLU())\n",
    "\n",
    "\n",
    "# Input with N * filters * (6,7)\n",
    "# Output with N * filters * (6,7)\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(filters, filters, 3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(filters, filters, 3, padding=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(filters)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(filters)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Input with N * filters * (6,7)\n",
    "# Output with N * 1 * 1\n",
    "class ValueHead_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueHead, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(filters, 1, 1)\n",
    "        self.batch_norm = nn.BatchNorm2d(1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fcN = nn.Sequential(*[nn.Linear(info.area, info.area) for _ in range(4)])\n",
    "        self.fc2 = nn.Linear(info.area, 1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.w1 = nn.Parameter(torch.tensor(1.0), requires_grad=False)\n",
    "        self.w2 = nn.Parameter(torch.tensor(0.5), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        x = self.fcN(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "#         map from [-1, 1] to [0, 1]\n",
    "        x = (x + self.w1) * self.w2\n",
    "        return x\n",
    "\n",
    "# Input with N * filters * (6,7)\n",
    "# Output with N * 3\n",
    "class ValueHead_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueHead_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(filters, 1, 1)\n",
    "        self.batch_norm = nn.BatchNorm2d(1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.fcN = nn.Sequential(*[nn.Linear(info.area, info.area) for _ in range(4)])\n",
    "        self.fc2 = nn.Linear(info.area, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        x = self.fcN(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, 3)\n",
    "        return x\n",
    "\n",
    "def init_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "    elif type(m) == nn.Conv2d:\n",
    "        nn.init.uniform_(m.weight)\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        nn.init.uniform_(m.weight)\n",
    "#         nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.0002)\n",
    "    elif type(m) == nn.Conv2d:\n",
    "        nn.init.normal_(m.weight, std=0.0002)\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        nn.init.normal_(m.weight, std=0.0002)\n",
    "#         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "net = nn.Sequential(convolutional_layer,\n",
    "                    nn.Sequential(*[ResidualLayer() for _ in range(4)]),\n",
    "                    ValueHead_Classifier())\n",
    "\n",
    "net.apply(init_normal)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()\n",
    "weight = torch.tensor([float(len(value_train) - len(value_train[value_train == i])) / len(value_train) for i in [1.0, 0.5, 0.0]])\n",
    "weight = weight.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    13] loss: 1.123\n",
      "Category, # Predictions, Accuracy of the network on the test:  0, 8876,  0\n",
      "Category, # Predictions, Accuracy of the network on the test:  1, 1295,  1295\n",
      "Category, # Predictions, Accuracy of the network on the test:  2, 3341,  0\n",
      "Example:\n",
      "value:   tensor(0, device='cuda:0') \n",
      "y:   tensor([0.3368, 0.3663, 0.2969], device='cuda:0') \n",
      "prediction:   tensor(1, device='cuda:0')\n",
      "[2,    13] loss: 1.119\n",
      "Category, # Predictions, Accuracy of the network on the test:  0, 8876,  0\n",
      "Category, # Predictions, Accuracy of the network on the test:  1, 1295,  1295\n",
      "Category, # Predictions, Accuracy of the network on the test:  2, 3341,  0\n",
      "Example:\n",
      "value:   tensor(0, device='cuda:0') \n",
      "y:   tensor([0.3396, 0.3620, 0.2985], device='cuda:0') \n",
      "prediction:   tensor(1, device='cuda:0')\n",
      "[3,    13] loss: 1.116\n",
      "Category, # Predictions, Accuracy of the network on the test:  0, 8876,  0\n",
      "Category, # Predictions, Accuracy of the network on the test:  1, 1295,  1295\n",
      "Category, # Predictions, Accuracy of the network on the test:  2, 3341,  0\n",
      "Example:\n",
      "value:   tensor(0, device='cuda:0') \n",
      "y:   tensor([0.3423, 0.3577, 0.3000], device='cuda:0') \n",
      "prediction:   tensor(1, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(n_epocs):\n",
    "    \n",
    "#     net = net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, d in enumerate(train_gen, 0):\n",
    "        # get the inputs\n",
    "        board, value = d\n",
    "#         print(board, value)\n",
    "\n",
    "        board, value = board.to(device), value.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(board)\n",
    "        loss = criterion(outputs, value)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        step = int(1.0 * len(train) / batch_size)\n",
    "        if i % step == step - 1:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / step))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # validate\n",
    "    with torch.set_grad_enabled(False):\n",
    "        net = net.eval()\n",
    "        correct = {i : 0 for i in range(3)}\n",
    "        total = {i : 0 for i in range(3)}\n",
    "        for board, value in test_gen:\n",
    "            board, value = board.to(device), value.to(device)\n",
    "\n",
    "            outputs = net(board)\n",
    "            y = nn.functional.softmax(outputs, dim=1)\n",
    "            prediction_class = torch.max(outputs, dim=1)[1]\n",
    "\n",
    "            if batch_size == 1:\n",
    "                total[value.item()] += 1\n",
    "                if categories == value:\n",
    "                    correct[value.item()] += 1 \n",
    "            else:\n",
    "                for k in correct:\n",
    "                    idx = (value == k).nonzero()\n",
    "                    total[k] += len(idx)\n",
    "                    correct[k] += len(torch.eq(prediction_class[idx], value[idx]).nonzero())\n",
    "            \n",
    "        for k in correct:\n",
    "            print('Category, # Predictions, Accuracy of the network on the test:  %s, %d,  %d' % (\n",
    "                k,\n",
    "                total[k],\n",
    "                correct[k]))\n",
    "# #             print('Average value predicted by the network: ')\n",
    "        print(\"Example:\\nvalue:  \", value[0], \"\\ny:  \", y[0], \"\\nprediction:  \", prediction_class[0])\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "with torch.set_grad_enabled(False):\n",
    "    net = net.eval()\n",
    "    correct = {i : 0 for i in [0, 0.5, 1]}\n",
    "    total = {i : 0 for i in [0, 0.5, 1]}\n",
    "    for board, value in test_gen:\n",
    "        board, value = board.to(device), value.to(device)\n",
    "\n",
    "        outputs = net(board)\n",
    "        categories = categorise_predictions(outputs)\n",
    "\n",
    "        for k in correct:\n",
    "            idx = (categories == k).nonzero()\n",
    "            total[k] += len(idx)\n",
    "            correct[k] += (categories[idx] == value[idx]).nonzero().sum().item()\n",
    "    for k in correct:\n",
    "        print('Category, # Predictions, Accuracy of the network on the test%s: %d %d %%' % (\n",
    "            k,\n",
    "            total[k],\n",
    "            (100 * float(correct[k]) / float(total[k])) if total[k] != 0 else 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)\n",
    "# save that crap\n",
    "torch.save({\n",
    "    'net_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "},\n",
    "    open('/home/richard/Downloads/nn.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively load it\n",
    "assert(False)\n",
    "checkpoint = torch.load('/home/richard/Downloads/nn.pth')\n",
    "net.load_state_dict(checkpoint['net_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
